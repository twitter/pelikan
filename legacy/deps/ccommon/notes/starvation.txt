One concern in an asynchronous, event driven programming model is that different request streams are not treated fairly. For example, a pipelined client with very high request rate can get better average response time compared to one with much lower rate, if the server always serves all pending requests from one fd before moving on to the next, due to uneven head-of-line blocking. The Memcached's way of addressing this is to cap the number of requests to be handled in each event, and 'yield' after that (by generating another event before quitting the event handler, which moves itself to the end of the event queue). Another solution, as suggestion by epoll, is to use a ready list to remember if there's data available on each fd, so the processing is decoupled from events.

The tradeoff between using event libraries' own event queues and explicitly managing our own queue is one between resource efficiency and flexibility. I think the ready list is probably more suitable in the long-term, especially if we come up with a multithreaded implementation and will support both simple and complex operations. However, initially I am going to ignore the starvation problem, especially given all our clients are currently using blocking calls, so no more than one outstanding request will exist in each fd.
