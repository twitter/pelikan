IO buffers:
IO buffer can be quite a memory hog. In Twemcache, each connection can use up to 27KB for buffering, meaning every 1000 connections (a small number in Twitter's SOA between two endpoints) enough memory to construct a hashtable with a few million keys. This reduces the scalability of a server in terms of how many concurrent clients it can host, and exposes vulnerability in terms of OOM risk when network glitches happen (connections are reset on client side but not server).

The question is, what size is reasonable? After all, IO on FDs are often some of the most expensive syscalls in a server. On the read side, we do want to read as much data as possible in one go. On the other hand, if the amount of outstanding data is small and the request sizes are also small, the actual amount of data to be read each time should also be small.

On the read path, the upper bound for per-connection userspace read buffer is the socket receive buffer (SO_RCVBUF), that's how much data can be returned in one read anyway. But we can use smaller per-connection buffer. Why? Because read requests are usually pretty small, no more than 1KB due to key size constraints, and when multiget uses high cardinality it's bad for other reasons such as high latencies. Write requests _can_ be much larger, but the protocols are often designed to allow zero copy- that is, we can copy the data directly into final destination, bypassing connection buffer altogether, Twemcache does this for storage requests. This means for each request we only are interested in storing the "header" part in connection buffer, so we can follow up with some parsing and direct copying. Given that, a number as low as 4KB seems more than enough.

On the write path, the socket send buffer is often much larger, esp. in high latency environment. Arguably this practice is unnecessary in data centers where everything is very close to each other, and potentially can be harmful even- a large number of large send buffers are an unintended DDoS attack waiting to be triggered by some unfortunate host/network conditions. Further more, the server probably should do zero copy on the portion of responses that are already stored somewhere in the server, meaning again connetion write buffer is only used to format the "response" header, if used at all. I would argue 1KB is more than enough to form several responses.

We should try allocating a single read buffer to avoid calling vector read or multiple reads. Write buffer can be much more scattered.
