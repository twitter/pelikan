Having an abstraction over communication channels (TCP, UDP, Unix Domain Socket, file, shared memory....) can isolate higher level logic from details specific to the channel media and peculiarities. For example, decisions like whether retry is needed, when and how, or even possible, should be contained with the module that understands the channels. Higher level logic only needs to know three responses: succeeded, delayed (retry needed), aborted, which correspond to three follow-up actions: proceed, wait, abort.

In the context of cache backends, there's some fundamental asymmetry in the interfaces for input and output.

For efficiency reasons, we may need more than one API for input. For example, read requests never result in storing anything in memory, except for metadata, and they are relatively short; write requests often come with payload of variable sizes that can result in writing to and allocating space for the payload. For the former, copy the requests from the underlying channel buffers (TCP socket buffer, for example) to userspace is not a big deal; for the latter, if the payloads are small, we can still afford to copy that into a buffer in userspace first, and copy again into their final stored location, however, for large payloads it will become increasingly desirable to achieve zero copy, by doing a lookup or allocation first and then copying the payload directly to the final stored location. So sometimes we want to copy data into "message buffers" that are often affiliated to each channel and managed there as well, in other cases we may have to interact with memory subsystems that are outside of the channels and message buffers, which may have a different layout as well. Specifically, in the current libccommon design, message buffers are mbuf queues, but data storage uses slabs/items. While it might be leaky abstraction to make the IO interface support writing into slabbed memory, it is not unreasonable to write into a "raw" memory area as an alternative- after all, this optimization is only worthwhile if we are copying large amount data into a location of large amount of consecutive memory.

Output looks yet again different. Most of the time, the responses that need to be sent back are already somewhere in memory- keys, values, error messages, but they need to be assembled according to the protocol specification, and some other fields need to be constructed along the process. So the best match is the sendmsg() model- a scatter-gather process that reads from an array of buffers, and the higher level logic needs to assemble this array before calling the lower level IO functions, because the buffers may not have the same format.

There's also inherent efficiency differences in seemingly symmetric IO APIs. writev, for example, simply incorporate multiple writes to a FD into a single function call; readv, however, involves allocating a temporary buffer (within readv) before copying the data into target vector as specified. So readv ends up being more expensive than read if there's only one element involved, while writev does not suffer from such inefficiency. This means from a performance perspective, we should try using read over readv if possible, and writev over write. This also answers stu's question of why we may want more than just a pair of vector IO. Of course, it is worth considering to make contiguous read a special case and optimize for it with read() under the API boundary.

On the other hand, if we are writing a proxy, IO become symmetric again, because data flowing in each direction has to go through both a read phase and a write phase, and never has to be stored anywhere other than in message buffers.
